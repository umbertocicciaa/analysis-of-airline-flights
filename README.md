# âœˆï¸ Analysis of airline flights

[![CI - Airflights](https://github.com/umbertocicciaa/analysis-of-airline-flights/actions/workflows/airflights.yml/badge.svg)](https://github.com/umbertocicciaa/analysis-of-airline-flights/actions/workflows/airflights.yml)

Analyze and visualize air traffic data at scale using a modern big data stack. This project demonstrates distributed processing with Hadoop and Spark, real-time analytics with Redis, and containerized orchestration via Docker.

---

## ğŸ“Š Demo

![Demo](resources/demo.png)

---

## ğŸš€ Features

- Distributed data processing with **Apache Spark**
- HDFS storage via **Hadoop**
- Data warehouse storage via **Hive**
- In-memory caching and messaging using **Redis**
- Scalable and isolated setup via **Docker Compose** or **K8S**

---

## ğŸ§± Project Structure

```bash
air-flights-big-data-unical/
â”œâ”€â”€ src/                    # Application code and environment settings
â”œâ”€â”€ run.sh                  # Script for local (non-Docker) execution
â”œâ”€â”€ docker-compose.yml      # Multi-service Docker configuration (DEV)
â”œâ”€â”€ docker-compose-prod.yml # Multi-service Docker configuration (PROD)
â”œâ”€â”€ src/Dockerfile          # Base image for application container
â””â”€â”€ resources/              # Static assets and demo images
```

---

## â˜¸ï¸ Run with K8s

### Prerequisites

- Kubernetes Cluster
- [kubectl](https://kubernetes.io/docs/reference/kubectl/)
- Dataset provided by the instructor

### Usage

This will spin up all required services (Hadoop, Hive, Spark, Redis, App) automatically.

```bash
chmod u+x install_cluster.sh
./install_cluster.sh install
```

Withot install input you destroy the namespace

```bash
chmod u+x install_cluster.sh
./install_cluster.sh
```

---

## ğŸ³ Run with Docker Compose

### Prerequisites

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- Dataset provided by the instructor

### Usage

```bash
docker compose -f docker-compose-prod.yaml
```

This will spin up all required services (Hadoop, Spark, Redis, App) automatically.

---

## ğŸ’» Run Locally (Manual Setup)

### Prerequisites

- Python 3.x
- Redis
- Apache Spark
- Apache Hadoop
- Java JDK (8 or 11)
- Dataset provided by the instructor

### Steps

```bash
# Set JAVA_HOME in src/local.env to your JDK 8 or 11 path
# Example: 
JAVA_HOME=/usr/lib/jvm/java-11-openjdk

# Make script executable and run
chmod u+x run_local.sh
./run_local.sh
```

---

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more information.

---
